My first attempt was reasonably fast: we loop through the grid, recursing only if we hit a splitting point. At a splitting point, we only recurse if we haven't seen that (point, direction) tuple before. I was keeping a cache of (point,direction) tuples and then at the end I was doing a `len(set([start for start, _ in cache]))` to count the number of unique starting points. It turned out that running this list comprehension on every recursion was actually quite slowâ€”who knew! 

I replaced the list comprehension by just keeping track of two different sets: one for points and one for point/direction tuples, then we can just do len(points) to get the length. This sped things up from 53 seconds to 3 seconds. I made a few micro-optimizations like removing unnecessary len calls and I switched the cache from two sets to one defaultdict where the keys are points and the values are sets of directions. The only tricky thing with this is you have to be careful:
doing `if dir_ in cache[point]` will actually *insert* the key `point` into `cache`, which could mess up my count of keys in the end. Instead, I do: `if dir in cache.get(point,[])` to avoid doing that extra key insertion.

I tried to do a version that cached outcomes *between* runs for part 2 (i.e. a cache from one starting point that can be reused for other starting points by keeping track of all the points you will energize if you start here and move in that direction), but this turned into a mess and didn't work. It requires recursing for every single step forward, so it would've likely hit python recursion  limits anyway.

Attempts to leverage the cache for *every* step (not just recursions into pipes) didn't seem to speed anything up, I'm not sure why.